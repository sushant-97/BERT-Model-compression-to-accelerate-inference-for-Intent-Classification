# Fine-Tuning BERT with LoRA for Stack Overflow Question Classification

## Advanced Model Optimization:
Conducted an extensive project focused on the compression of BERT for Intent Classification. Employed weight pruning, quantization, and knowledge distillation techniques to create an efficient model with significantly reduced resource demands, making it well-suited for production deployment.

## Optimal Performance:
Successfully achieved reduced latency and minimized model size by reducing the number of parameters while maintaining high accuracy levels. This meticulous optimization process ensured efficient and effective model performance, ideal for real-world applications.

## Strategic Resource Utilization:
The project's outcomes highlighted the strategic utilization of resources, making it possible to harness the full potential of deep learning models while minimizing computational overhead, ultimately benefiting various inference applications.

##### Deep-Learning-Model-Compression-using-Pruning
Successfully implemented weight pruning on a Keras-tensorflow model trained for syntactic analysis using LSTM over the IMDB datasets.
Achieved a remarkable 70% model compression with only a 1.2% drop in accuracy, demonstrating a significant reduction in the model's size and computational requirements.
Conducted thorough experimentation and fine-tuning to determine the optimal pruning parameters, resulting in the highest possible compression while maintaining satisfactory accuracy.
Demonstrated proficiency in using state-of-the-art deep learning techniques to optimize model performance and efficiency.
Contributed to the development of cutting-edge natural language processing (NLP) technology through the application of advanced machine learning techniques.
Collaborated effectively with team members to achieve project objectives and meet project timelines.
Documented experimental procedures, results, and conclusions in a clear and concise manner to facilitate knowledge transfer and future development.
